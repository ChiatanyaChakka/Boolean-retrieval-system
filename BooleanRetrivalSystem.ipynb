{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d2de19",
   "metadata": {},
   "source": [
    "D:\\All Acads\\3-2\\Information Retrival\\Assignment Files\\Document_Corpse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29770394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.corpus import words\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b620737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(str, n):\n",
    "    \"\"\"\n",
    "    Left rotates the given string by n times\n",
    "    :param str: the string that needs to be rotated\n",
    "    :param n: the number of times the string needs to be left rotated\n",
    "    :returns: the rotated string\n",
    "    \"\"\"\n",
    "    return str[n:] + str[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1b716c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(list1, list2):\n",
    "    \"\"\"\n",
    "    performs intersection of two given lists.\n",
    "    \n",
    "    :param list1: list of docIDs\n",
    "    :param list2: list of docIDs\n",
    "    :returns: intersection of the two given lists\n",
    "    \"\"\"\n",
    "    final_list=[]\n",
    "    for docID in list1:\n",
    "        if docID in list2:\n",
    "            final_list.append(docID)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56409db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR(list1, list2):\n",
    "    \"\"\"\n",
    "    performs union of two given lists.\n",
    "    \n",
    "    :param list1: list of docIDs\n",
    "    :param list2: list of docIDs\n",
    "    :returns: union of the two given lists\n",
    "    \"\"\"\n",
    "    final_list = list(set(list1) | set(list2))\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62bad34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT(list1):\n",
    "    \"\"\"\n",
    "    returns the items that are not there in this list\n",
    "    \n",
    "    :param list1: list of docIDs\n",
    "    :returns: list of items that are not present in this list\n",
    "    \"\"\"\n",
    "    total_list = list(range(42))\n",
    "    return [i for i in total_list if i not in list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ae69911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(listOfWords):\n",
    "    \"\"\"\n",
    "    Removes the stop words that have been scrapped from NLTK library.\n",
    "    \n",
    "    :param listOfWords: The total list of words present in the document\n",
    "    :returns: The list in which the stopwords have been removed\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word.lower() for word in listOfWords if not word.lower() in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1bb66b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(listOfWords):\n",
    "    \"\"\"\n",
    "    Performs stemming on each of the given list of words\n",
    "    \n",
    "    :param listOfWords: a list of words on which the stemming needs to be performed\n",
    "    :returns: a list of stemmed words\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(w) for w in listOfWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7fd9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(word, all_words):\n",
    "    \"\"\"\n",
    "    Corrects the given word if it has a spelling mistake. The list of correct words is  scrapped from NLTK library\n",
    "    \n",
    "    :param word: the word that might need correction\n",
    "    :returns: corrected word if the given word is spelled wrong\n",
    "    \"\"\"\n",
    "    correct_words = all_words\n",
    "\n",
    "    all_edit_distances = [(edit_distance(word, w),w) for w in correct_words if w[0]==word[0]]\n",
    "    return sorted(all_edit_distances, key = lambda val:val[0])[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8d9da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wildCardSearch(word, permutermIndex, invertedIndex):\n",
    "    \"\"\"\n",
    "    wildcard query terms can have many possibilites based on the document corpse.\n",
    "    This function takes the wildcard terms and searches for all the possiblites of the word and returns a combined list of \n",
    "    documents that the possible words are present\n",
    "    \n",
    "    :param word: the wildcard query term\n",
    "    :param permutermIndex: the permuterm index which has all the words with their possible left rotations\n",
    "    :param invertedIndex: the inverted index for searching the possible word postings\n",
    "    :returns: the postings of union of all possible words\n",
    "    \"\"\"\n",
    "    possibilties = []\n",
    "    dword = word + '$'\n",
    "    while dword[-1] != '*':\n",
    "        dword = rotate(dword, -1)\n",
    "    dword = dword[:-1]\n",
    "    for word, permuterms in permutermIndex.items():\n",
    "        for permutern in permuterms:\n",
    "            if permutern.find(dword) == 0:\n",
    "                possibilties.append(word)\n",
    "                break\n",
    "    finalList = []\n",
    "    for word in possibilties:\n",
    "        finalList = OR(finalList, invertedIndex[word])\n",
    "    return finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a0f2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceByLists(listOfTokens, invertedIndex, permutermIndex, all_words):\n",
    "    \"\"\"\n",
    "    On tokenizing the query, we will have the query terms present in a list of tokens. These can be replaced by their postings\n",
    "    and this function performs that\n",
    "    \n",
    "    :params listOfTokens: the list of tokens in the query\n",
    "    :params invertedIndex: the inverted index data structure\n",
    "    :params permutermIndex: the permuterm index data structure\n",
    "    \"\"\"\n",
    "    for index, token in enumerate(listOfTokens):\n",
    "        if token not in ['&', '~', '|', \"(\", \")\"]:\n",
    "            if '*' in token:\n",
    "                listOfTokens[index] = wildCardSearch(token, permutermIndex, invertedIndex)\n",
    "            else:\n",
    "                token = token.lower()\n",
    "                token = correction(token, all_words)\n",
    "                token = stemming([token])[0]\n",
    "                listOfTokens[index] = invertedIndex[token]\n",
    "    return listOfTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5ae47c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolveSubProblem(listOfTokens):\n",
    "    \"\"\"\n",
    "    resloves tokens of the form list1 operator list2 where each list1  and list2 are postings while operator can be AND or OR\n",
    "    \n",
    "    :params listOfTokens: the tokens of the subproblem considered\n",
    "    \"\"\"\n",
    "    resolved = []\n",
    "    while '~' in listOfTokens:\n",
    "        indexNot = listOfTokens.index('~')\n",
    "        negatedList = NOT(listOfTokens[indexNot+1])\n",
    "        listOfTokens = listOfTokens[:indexNot] + listOfTokens[indexNot+2:]\n",
    "        listOfTokens.insert(indexNot, negatedList)\n",
    "    if listOfTokens[1] == \"&\":\n",
    "        resolved = AND(listOfTokens[0], listOfTokens[2])\n",
    "    else:\n",
    "        resolved = OR(listOfTokens[0], listOfTokens[2])\n",
    "    return resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dccfa4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateInvertedIndex(invertedIndex, wordsList, docID):\n",
    "    \"\"\"\n",
    "    Updates the inverted Index with the given new list of words\n",
    "    \n",
    "    :param permutermIndex: the old inverted index\n",
    "    :param wordsList: list of new words to be indexed\n",
    "    :param docID: ID of the document from which these words have been taken\n",
    "    :returns: the updated inverted index\n",
    "    \"\"\"\n",
    "    for word in wordsList:\n",
    "        if word not in invertedIndex:\n",
    "            invertedIndex[word] = [docID]\n",
    "        else:\n",
    "            if invertedIndex[word][-1] != docID:\n",
    "                invertedIndex[word].append(docID)\n",
    "    return invertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ea5f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatePermutermIndex(permutermIndex, wordsList):\n",
    "    \"\"\"\n",
    "    Updates the permuterm Index with the given new list of words\n",
    "    \n",
    "    :param permutermIndex: the old permuterm index\n",
    "    :param wordsList: list of new words to be indexed\n",
    "    :returns: the updated permuterm list\n",
    "    \"\"\"\n",
    "    for word in wordsList:\n",
    "        if word not in permutermIndex:\n",
    "            dkey = word + \"$\"\n",
    "            permutermIndex[word] = []\n",
    "            for i in range(len(dkey),0,-1):\n",
    "                permutermIndex[word].append(rotate(dkey,i))\n",
    "    return permutermIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f994a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessAndBuild(documentNames, corpseLocation):\n",
    "    \"\"\"\n",
    "    Performs pre procressing techniques on the given dataset/document corpse and builds inverted index and permuterm index\n",
    "    Pre-processing techniques performed:\n",
    "        1. Stopword removal\n",
    "        2. Stemming\n",
    "        3. Miscellaneous. Includes removing of non significant symbols like =, /, and .\n",
    "    \n",
    "    :param documentNames: The list of document names that this system will interact with\n",
    "    :param corpseLocation: The absolute location of the folder in which all the documents are present\n",
    "    :returns: Two data structures. Inverted index and Permuterm index\n",
    "    \"\"\"\n",
    "    \n",
    "    invertedIndex = {}\n",
    "    permutermIndex = dict()\n",
    "    all_words = []\n",
    "    for docID, doc in enumerate(documentNames):\n",
    "        # open the document and read its contents\n",
    "        document = open(corpseLocation + \"\\\\\" + doc)\n",
    "        doc_data = document.read()\n",
    "        document.close()\n",
    "        \n",
    "        # Remove miscellaneous symbols\n",
    "        doc_data = doc_data.replace(\"=\", \"\")\n",
    "        doc_data = doc_data.replace(\".\", \" \")\n",
    "        doc_data = doc_data.replace(\"/\", \" \")\n",
    "        \n",
    "        # Tokenize the document contents for preprocessing\n",
    "        words = word_tokenize(doc_data)\n",
    "        \n",
    "        # Remove stop words\n",
    "        filtered_words = removeStopWords(words)\n",
    "        \n",
    "        all_words += filtered_words\n",
    "        \n",
    "        # Perform stemming on each word\n",
    "        stemmed_words =  stemming(filtered_words)\n",
    "        \n",
    "        # Update the indexes with the new set of words\n",
    "        invertedIndex = updateInvertedIndex(invertedIndex, stemmed_words, docID)\n",
    "        permutermIndex = updatePermutermIndex(permutermIndex, stemmed_words)\n",
    "        \n",
    "    return (invertedIndex, permutermIndex, all_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd035d",
   "metadata": {},
   "source": [
    "Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f0d0b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the absolute path to the document corpseD:\\All Acads\\3-2\\Information Retrival\\Assignment Files\\Document_Corpse\n",
      "Pre-processing and building index please wait...\n",
      "Pre-processing done!\n",
      "Time Lapsed = 0:0:17.011948823928833\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "path_to_doc_corpse = input(\"Enter the absolute path to the document corpse\")\n",
    "os.chdir(path_to_doc_corpse)\n",
    "documentNames = [doc for doc in os.listdir() if doc.endswith(\".txt\")]\n",
    "\n",
    "print(\"Pre-processing and building index please wait...\")\n",
    "start_time = time.time()\n",
    "invertedIndex, permutermIndex, all_words = preProcessAndBuild(documentNames, path_to_doc_corpse)\n",
    "end_time = time.time()\n",
    "print(\"Pre-processing done!\")\n",
    "sec = end_time-start_time\n",
    "mins = sec // 60\n",
    "sec = sec % 60\n",
    "hours = mins // 60\n",
    "mins = mins % 60\n",
    "print(\"Time Lapsed = {0}:{1}:{2}\".format(int(hours),int(mins),sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62cc344e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query( paul & dream )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query cell\n",
    "query = input(\"Enter your query\")\n",
    "queryTokens = query.split()\n",
    "# print(queryTokens)\n",
    "queryTokens = replaceByLists(queryTokens, invertedIndex, permutermIndex, all_words)\n",
    "while len(queryTokens) != 1:\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for j in range(len(queryTokens)):\n",
    "        if queryTokens[j] == ')':\n",
    "            i = j-1\n",
    "            while queryTokens[i] != '(':\n",
    "                i -= 1\n",
    "            resolved = resolveSubProblem(queryTokens[i+1:j])\n",
    "            queryTokens = queryTokens[:i] + queryTokens[j+1:]\n",
    "            queryTokens.insert(i, resolved)\n",
    "            break\n",
    "queryTokens[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
